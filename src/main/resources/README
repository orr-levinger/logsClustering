Complexity:
let N be the number of lines and W the number of word in the longest line and C the number of clusters.
than the complexity is O(NWC)
to reduce that complexity:

1. i could split the clusters into 2 groups one that "settled" its indexes and one that still having index=0
 and than we can first run in parallel over the the settled ones since there is no risk of collision.
 and only if we found no matches to run sequentially over the clusters that still have index = 0.
 since most of culsters should be settled than we could get O(NW)

2. to reduce W by hashing the whole sentence without parsing the new log line, just removing the different word and
comparing hashes. this is implemented by HashCluster resulting in O(NC).

3. so if i added also 1 than we would get O(N) witch is pretty awesome!!


for scaling this:
for scalding i would keep the services stateless and keep all the data in a scalable data base.
the logs the changing words and the list for clusters.
keep a reference from the logs and words to there's cluster ids..
that way i can deploy as many instances of the Cluster Service and use a framework like URIKA to load balance the load
of request. i will have to fugure out how to sync by verifying when i put updates to tables that they havnt changed since i did the GET
i can achive that by keeping a version of etch record and if i try to PUT a record the database will verify that the record wasnt changed by a different instanse




